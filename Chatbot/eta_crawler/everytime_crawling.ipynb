{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "#import requests #request+bs4 조합만으로도 crawling가능\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "#import re\n",
    "#import pandas as pd\n",
    "import getpass\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_url = 'https://everytime.kr/382452' #에타 조교 게시판 base_url\n",
    "#base_url = 'https://everytime.kr/375372/text/이중' #에타 이중전공/전과 게시판에서 '이중'으로 검색\n",
    "base_url = 'https://everytime.kr'\n",
    "login_url = 'https://everytime.kr/login' #에타 들어가려면 로그인을 해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver 실행\n",
    "def drive(url):\n",
    "    driver = webdriver.Chrome('./chromedriver') #driver 객체 불러옴\n",
    "    driver.implicitly_wait(3) # 3초 후에 작동하도록\n",
    "    driver.get(url) #url에 접속\n",
    "    html = driver.page_source #현재 driver에 나타난 창의 page_source(html) 가져오기\n",
    "    soup = BeautifulSoup(html, 'html.parser') #html 파싱(parsing)을 위해 BeautifulSoup에 넘겨주기\n",
    "    return driver, soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login\n",
    "def login():\n",
    "    login_url = 'https://everytime.kr/login'\n",
    "    driver, _ = drive(login_url)\n",
    "    _id = input('id: ')\n",
    "    _pw = getpass.getpass()\n",
    "    driver.find_element_by_name('userid').send_keys(_id)\n",
    "    driver.find_element_by_name('password').send_keys(_pw)\n",
    "    driver.find_element_by_xpath('//*[@id=\"container\"]/form/p[3]/input').click()\n",
    "    return driver\n",
    "\n",
    "#qt를 이용해서 login할 때, 인자를 따로 받아줌\n",
    "def login_qt(_id, _pw):\n",
    "    login_url = 'https://everytime.kr/login'\n",
    "    driver, _ = drive(login_url)\n",
    "    driver.find_element_by_name('userid').send_keys(_id)\n",
    "    driver.find_element_by_name('password').send_keys(_pw)\n",
    "    driver.find_element_by_xpath('//*[@id=\"container\"]/form/p[3]/input').click()\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get soup, 해당 페이지의 soup만 가져오는 함수\n",
    "def get_soup(driver):\n",
    "    body = driver.find_element_by_tag_name('body')\n",
    "    num_page_down = 1\n",
    "    while num_page_down:\n",
    "        body.send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(1.5)\n",
    "        num_page_down -= 1\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_board_names():\n",
    "    driver = login()\n",
    "    soup = get_soup(driver)\n",
    "    driver.close()\n",
    "    board = []\n",
    "    board_tags = soup.select(\"#submenu\")[0].select('a')\n",
    "\n",
    "    for b in board_tags:\n",
    "        board.append((b.text,b.get('data-id')))\n",
    "        board_dic = dict(board)\n",
    "    return board_dic\n",
    "\n",
    "def save_board_names(board_dic):\n",
    "    data = '' #여기에 게시판이름 \\t 게시판id \\n 식으로 텍스트를 넣을 것임.\n",
    "    b_list = list(board_dic)\n",
    "    for i in b_list:\n",
    "        try:\n",
    "            data += i + '\\t' + board_dic[i] + '\\n' #데이터 입력\n",
    "        except:\n",
    "            continue #게시판id에 None type이 올 수도 있기에 이 경우에는 그냥 넘긴다. (게시판 찾기의 경우임)\n",
    "    f = open('게시판명.txt', 'w', encoding = 'utf8')\n",
    "    f.write(data)\n",
    "    print('파일이 작성되었습니다.')\n",
    "    f.close()\n",
    "    \n",
    "def load_board_names(file_name):\n",
    "    f = open(file_name, 'r', encoding = 'utf8')\n",
    "    data = f.read()\n",
    "    board_list = data.split('\\n')\n",
    "    b_tuple_list = []\n",
    "    for b in board_list:\n",
    "        try:\n",
    "            b_tuple_list.append((b.split('\\t')[0], b.split('\\t')[1])) #마지막 줄은 ''이어서 split 후 index로 참조 못하기에 try문으로 넘어가도록 한다. \n",
    "        except:\n",
    "            continue\n",
    "       \n",
    "    board_dic = dict(b_tuple_list)\n",
    "    f.close()\n",
    "    return board_dic\n",
    "\n",
    "def board_selection(board_dic, board_name): #게시판 선택\n",
    "    global base_url\n",
    "    base_url = base_url + '/' +board_dic[board_name]\n",
    "    \n",
    "def board_query(board_dic, query): #게시판 내 질문 선택\n",
    "    global base_url\n",
    "    base_url = base_url + '/all/' +query\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "board_dic = get_board_names()\n",
    "\n",
    "save_board_names(board_dic)\n",
    "\n",
    "board_dic = load_board_names('게시판명.txt')\n",
    "\n",
    "board_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 게시판 글 정보 크롤링 함수, 글과 댓글 크롤링 위한 기초 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class counsel_data:\n",
    "    def __init__(self, driver):\n",
    "        global base_url\n",
    "        #self.base_url = 'https://everytime.kr/382452'\n",
    "        self.writer_list = []\n",
    "        self.more_list = []\n",
    "        self.content_list = []\n",
    "        self.id_list = []\n",
    "        self.time_list = []\n",
    "        self.vote_list = []\n",
    "        self.comment_list = []\n",
    "        self.driver = driver\n",
    "        self.driver.get(base_url)\n",
    "        \n",
    "        \n",
    "    def check_rep(self, id_list):\n",
    "        rep_idx = []\n",
    "        for i in id_list:\n",
    "            if i in self.id_list:\n",
    "                rep_idx.append(id_list.index(i))\n",
    "        return rep_idx\n",
    "    \n",
    "    def get_soup(self):\n",
    "        body = self.driver.find_element_by_tag_name('body')\n",
    "        num_page_down = 1\n",
    "        \n",
    "        while num_page_down:\n",
    "            body.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(1.5)\n",
    "            num_page_down -= 1\n",
    "            \n",
    "        html = self.driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    def get_counsel_data(self):\n",
    "        soup = self.get_soup()\n",
    "\n",
    "        counsel = soup.select('.wrap.articles')\n",
    "        #글쓴이\n",
    "        writer_list = list(map(lambda x: x.text, counsel[0].select('h3')))\n",
    "\n",
    "        #더보기 여부\n",
    "        temp_list = counsel[0].select('a[class=\"article\"]')\n",
    "        more_list = []\n",
    "        for i in temp_list:\n",
    "            if i.select('.more'):\n",
    "                more_list.append(1)\n",
    "            else:\n",
    "                more_list.append(0)\n",
    "        #글 내용\n",
    "        content_list = list(map(lambda x: x.text, counsel[0].select('p')))\n",
    "\n",
    "        #글 접근 위한 id주소\n",
    "        id_list = list(map(lambda x: x.get('name').split('_')[0], counsel[0].select('input[name]')))\n",
    "        if len(id_list)>20:\n",
    "            id_list.pop()\n",
    "            \n",
    "\n",
    "        #글 작성 시간\n",
    "        time_list = list(map(lambda x: x.text, counsel[0].select('time')))\n",
    "\n",
    "        #좋아요 수\n",
    "        vote_list = list(map(lambda x: x.text, counsel[0].select('.vote')))\n",
    "\n",
    "        #댓글 수\n",
    "        comment_list = list(map(lambda x: x.text, counsel[0].select('.comment')))\n",
    "        \n",
    "        \n",
    "        \n",
    "        return writer_list, more_list, content_list, id_list, time_list, vote_list, comment_list\n",
    "    \n",
    "    #크롤링 시작\n",
    "    def start_crawling(self):\n",
    "        start_idx = 1\n",
    "        writer_list, more_list, content_list, id_list, time_list, vote_list, comment_list = self.get_counsel_data()\n",
    "        self.writer_list += writer_list\n",
    "        self.more_list += more_list\n",
    "        self.content_list += content_list\n",
    "        self.id_list += id_list\n",
    "        self.time_list += time_list\n",
    "        self.vote_list += vote_list\n",
    "        self.comment_list += comment_list\n",
    "        \n",
    "        while True:\n",
    "            start_idx+=1\n",
    "            if start_idx%10 ==0:\n",
    "                print(start_idx)\n",
    "            self.driver.get(base_url+f'/p/{start_idx}')\n",
    "            try:\n",
    "                writer_list, more_list, content_list, id_list, time_list, vote_list, comment_list = self.get_counsel_data()\n",
    "                \n",
    "                self.writer_list += writer_list\n",
    "                self.more_list += more_list\n",
    "                self.content_list += content_list\n",
    "                self.id_list += id_list\n",
    "                self.time_list += time_list\n",
    "                self.vote_list += vote_list\n",
    "                self.comment_list += comment_list\n",
    "            except:\n",
    "                print('finished')\n",
    "                self.driver.close()\n",
    "                break\n",
    "            if not writer_list: #가져 온 데이터가 아무 것도 없으면 page 색인 종료\n",
    "                print('finished')\n",
    "                break\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 게시판 고르고 로그인 후 class 생성\n",
    "board_dic = load_board_names('게시판명.txt')\n",
    "\n",
    "for b in board_dic:\n",
    "    print(b)\n",
    "    \n",
    "board_name = input('크롤링하고 싶은 게시판명을 골라주세요: ')\n",
    "board_selection(board_dic, board_name)\n",
    "\n",
    "driver = login()\n",
    "cns_data = counsel_data(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤링\n",
    "cns_data.start_crawling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 댓글 크롤링 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(driver, ids):\n",
    "    ids = ids\n",
    "    #반환 값이 담길 리스트들\n",
    "    articles = []\n",
    "    comments = []\n",
    "    comments_writers = [] \n",
    "    comments_tag = []\n",
    "    \n",
    "    max_len = len(ids)\n",
    "    proceed = -1\n",
    "    \n",
    "    for n,i in enumerate(ids):\n",
    "        percentage = int((n/max_len)*100)\n",
    "        if percentage%10 == 0 and percentage>proceed:\n",
    "            print(percentage)\n",
    "            proceed = percentage\n",
    "            time.sleep(3)\n",
    "            \n",
    "        driver.get(base_url+f'/v/{i}')\n",
    "        soup = get_soup(driver)\n",
    "        \n",
    "        page_info = soup.select('.wrap.articles')[0]\n",
    "        \n",
    "        temp_art = list(map(lambda x: x.text, page_info.select('.article')[0].select('p')))\n",
    "        articles+=temp_art\n",
    "        \n",
    "        \n",
    "        #댓글 정보들\n",
    "        comments_info = page_info.select('.comments')[0]\n",
    "        \n",
    "        #댓글 \n",
    "        temp_comments = list(map(lambda x: x.text, comments_info.select('.large')))\n",
    "        if temp_comments: #댓글이 있으면 추가한다.\n",
    "            comments.append(temp_comments)\n",
    "        else:\n",
    "            comments+=[None]\n",
    "        \n",
    "        #댓글 작성자\n",
    "        temp_comments_writers = list(map(lambda x: x.text, comments_info.select('h3')))\n",
    "        if comments_writers:\n",
    "            comments_writers.append(temp_comments_writers)\n",
    "        else:\n",
    "            comments_writers += [None]\n",
    "                \n",
    "        # 댓글 대댓글 여부\n",
    "        if comments_info.select('article'):\n",
    "            temp_comments_tag = list(map(lambda x: x.get('class')[0], comments_info.select('article')))\n",
    "            comments_tag.append(temp_comments_tag)\n",
    "            \n",
    "        else:\n",
    "            #댓글이 없으면 그냥 None 넣어줌\n",
    "            \n",
    "            comments_tag += [None]\n",
    "        \n",
    "        \n",
    "    return articles, comments,  comments_writers, comments_tag, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(cns_data.id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤러 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawling 1000개씩 나누기\n",
    "def crawl_devision(cns_data):\n",
    "    ids = copy.deepcopy(cns_data.id_list)\n",
    "    n = len(ids)//1000 #n*1000개의 data, 1000보다 작으면 그냥 바로 계산\n",
    "    if n<1: #1000개보다 적다.\n",
    "        return 0, [ids] #0은 n값\n",
    "    else: \n",
    "        ids_n = []\n",
    "        for i in range(n+1):\n",
    "            if i == n: #마지막 자투리\n",
    "                ids_n.append(ids[:len(ids)])\n",
    "            else:\n",
    "                ids_n.append(ids[i*1000:(i+1)*1000])\n",
    "        return n, ids_n\n",
    "\n",
    "def crawl_ntimes(n, ids):\n",
    "    n = n+1 #n=0이면 1번 크롤링, n=1이면 2번 크롤링...\n",
    "    articles = []\n",
    "    comments = []\n",
    "    comments_writers = []\n",
    "    comments_tag = []\n",
    "    ids_result = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        driver = login()\n",
    "        articles_t, comments_t,  comments_writers_t, comments_tag_t, ids_t = get_comments(driver, ids[i])\n",
    "        print(i)\n",
    "        articles += articles_t\n",
    "        comments += comments_t\n",
    "        comments_writers += comments_writers_t\n",
    "        comments_tag += comments_tag_t\n",
    "        ids_result += ids_t\n",
    "            \n",
    "    \n",
    "    driver.close()\n",
    "    return articles, comments, comments_writers, comments_tag, ids_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n, ids = crawl_devision(cns_data)\n",
    "\n",
    "articles, comments, comments_writers, comments_tag, ids_result = crawl_ntimes(n, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미리 크롤링한 id들을 deepcopy\n",
    "\n",
    "ids = copy.deepcopy(cns_data.id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출할 양이 적으면 그냥 실행(1000개 이하)\n",
    "\n",
    "ids4 = ids[:len(cns_data.id_list)]\n",
    "\n",
    "driver = login()\n",
    "\n",
    "articles, comments,  comments_writers, comments_tag, ids = get_comments(driver, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(len(articles))\n",
    "print(len(comments))\n",
    "print(len(comments_writers))\n",
    "print(len(comments_tag))\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pandas import Series, DataFrame\n",
    "\n",
    "raw_data = {'내용': articles,\n",
    "            '댓글': comments,\n",
    "           '댓글쓴이':comments_writers,\n",
    "           '대댓글여부':comments_tag,\n",
    "           'id':ids_result,}\n",
    "\n",
    "data = DataFrame(raw_data)\n",
    "data[30:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv로 저장\n",
    "\n",
    "data.to_csv(\"이중전과게시판_키워드_이중.csv\", mode='w', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
